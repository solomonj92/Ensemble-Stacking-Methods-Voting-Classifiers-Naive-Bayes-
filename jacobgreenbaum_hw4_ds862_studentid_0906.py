# -*- coding: utf-8 -*-
"""JacobGreenbaum_HW4_DS862_StudentID_0906

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ouf-eMhUC7s84KDQxOcQ6hk7k4Mxrett?resourcekey=0-BTwReYJIJ0-P4ld-DNYXGA
"""

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import CategoricalNB
!pip install mixed-naive-bayes
from mixed_naive_bayes import MixedNB
from pandas.api.types import is_numeric_dtype
from sklearn.preprocessing import StandardScaler
import itertools
from sklearn.model_selection import cross_validate
le = LabelEncoder()

# Read in the data
data = pd.read_csv('gcredit.csv')

# Check out first 5 records
data.head()

# Separate data into features and response
y = data.V21
del data['V21']

# Create a function to help split datatypes for the Niave Bayes exersizes
def d_type_split_num_str(df):
    """ 
    This function takes in a single dataframe and splits them into two dataframes. 
    One dataframe with all of the numeric features, also scales the data with standard scalar. 
    The other dataframe is treated as categorical variables that are fit and transformed using LabelEncoder().
    The function also returns a list of the columns index whose dtype = string
    Lastly, a dataframe which combines both the numeric (with StandardScaler()) and categorical variables  (with LabelEncoder)
    Args:
    df (pd.DataFrame): Dataframe to cast
    Returns:
    df[is_num]: all columns are numeric and fit_transformed using StandardScaler()
    df[is_str]: all columns are categorical varibles that are fit and transformed using LabelEncoder()
    str_index: list of indecies whose columns are dtype = string
    """
    
    num = [is_numeric_dtype(df[x]) for x in df.columns] # Check to see if each features data type is numeric
    lst = zip(num,df.columns) # Zip the result of is_numeric_dtype (True or False) with the feature name
    cols = list(lst) # Make a list with the data above
    is_num = [] # Empty string for the names of numeric features
    is_str = [] # Empty string for the names of categorical features
    str_index = [] # Empty string for the index of categorical features
    iter = 0 # Initiate an iterable for indexing purposes
    mixed_df = pd.DataFrame() # Empty dataframe for the mixed data dataframe
    for x in cols:
        if x[0] == True:
            is_num.append(x[1])
            iter += 1
            df[x[1]] = StandardScaler().fit_transform(np.array(df[x[1]]).reshape(-1,1))
            mixed_df[x[1]] =  df[x[1]]
        else:
            df[x[1]] = le.fit_transform(df[x[1]]) # Encode labels of categorical data
            mixed_df[x[1]] = df[x[1]]
            is_str.append(x[1])
            str_index.append(iter)
            iter += 1
    StandardScaler().fit_transform(mixed_df[is_num])
    return StandardScaler().fit_transform(df[is_num]), df[is_str],str_index, mixed_df

data_num, data_cat, cat_index, mixed_df = d_type_split_num_str(data)


X_train_cat, X_test_cat, y_train, y_test = train_test_split(data_cat, y, test_size=0.2, random_state = 862) # Split the cateogical data for training and testin
X_train_num, X_test_num, y_train, y_test = train_test_split(data_num, y, test_size=0.2, random_state = 862) # Split the numeric data for training and testin

# First we will try and create a model consisting of only the categorical features using CategoricalNB(), 
#  one with only numeric using GaussianNB(), 
#  and then one with both numeric and categorical using MixedNB()

# Create a GridSearchCV() for hyperparameter tuning for CategoricalNB models
cat_pipe = Pipeline([('clf',CategoricalNB())])
cat_search = {'clf__alpha' : np.geomspace(.000001,100,1000)}

# Initiate the model
cat_clf = GridSearchCV(cat_pipe, cat_search, cv=5, n_jobs=-1)
cat_clf.fit(X_train_cat, y_train)  # Fit the models

# Results of the CV, best hyperparameters
print(cat_clf.best_params_) #{'clf__alpha': 1e-06}
# Best training score
print(cat_clf.best_score_) #.74375
# Test score for CategoricalNB()
print(cat_clf.score(X_test_cat, y_test)) #.755

# Create a GridSearchCV() for hyperparameter tuning for GaussianNB models
##### Naive Bayes for continuous features
num_pipe = Pipeline([('clf',GaussianNB())])
num_search = {'clf__var_smoothing' :  np.logspace(0,-9, num=100)}

# Initiate the model
num_clf = GridSearchCV(num_pipe, num_search, cv=5, n_jobs=-1)
num_clf.fit(X_train_num, y_train) # Fit the models

# Results of the CV, best hyperparameters
print(num_clf.best_params_) # {'clf__var_smoothing': 0.3511191734215131}
# Best training score
print(num_clf.best_score_) #.716249
# Test score for GaussianNB()
print(num_clf.score(X_test_num, y_test)) #.71

# Now we will fit a model for Mixed data types niave bayes
##### Mixed Niave Bayes, categorical and continuous 

y = le.fit_transform(y) # Transform the response variable for MixedNB

# Split the mixed dataframe into training and testing sets
X_train_mixed, X_test_mixed, y_train, y_test = train_test_split(mixed_df, y, test_size=0.2, random_state = 862)

# Define hyperparameter search space
alpha_vals = np.geomspace(.0000001,100,50).astype(float)
var_smoothing_vals = np.logspace(0,-9, num=50).astype(float)

# Create a list which holds each of the combinations of hyperparameters
combos = list(itertools.product(alpha_vals, var_smoothing_vals))

params = [] # Empty list for the validation scores
k = 5 # Create a 5 fold CV
for a, s in combos: 
    clf = MixedNB(categorical_features = cat_index, alpha = a, var_smoothing = s) # Initiate the model
    cv_results = cross_validate(clf, X_train_mixed, y_train, cv=k) # Initiate the cross validation
    params += [np.mean(cv_results["test_score"])] # Add the mean CV scores to the list, params

best_score_idx = np.argmax(params) # Find the index of the best score
params[best_score_idx] # best average of the 5 cross validation scores = .7625, this is the best training score between the three models
best_alpha, best_var_smoothing = combos[best_score_idx] # best_alpha = 1e-07 best_var_smoothing = 0.07906043210907697)

clf = MixedNB(categorical_features = cat_index, alpha = best_alpha, var_smoothing = best_var_smoothing) # Initiate the model with the best hyperparameters
clf.fit(X_train_mixed, y_train) # retrain the model with best hyperparameters
print('Training Score: ',clf.score(X_train_mixed, y_train)) # best training score between the three models at .7825
print('Test Score: ',clf.score(X_test_mixed, y_test)) # .74 test score which is actually .015 less accurate than using just categorical varriables

# Machine learning continuously surprises me. Findings:
#     1.  The model with only categorical variables has the best score 
#     2.  The model with only categorical variables has a test score that is better than the training score
#     3.  The model with both categorical and continuous variables has a training score of .7825 and a test score of .74
#         This could be because the additional variables is slightly overfitting the data
# Overall I think Niave Bayes does well on this data set but it is clear from the finicky results that Niave Bayes is infact Niave.
# Choosing the correct features is crucial for this type of model as it could be quite prone to overfitting

# For this assignment, you will be doing ensemble with classification instead. Almost all the functions that we have covered have a classification version that you can directly apply (with a few changes ocassionally, of course).
# 
# The dataset you will be using is a bank churn modeling found on [Kaggle](https://www.kaggle.com/shrutimechlearn/churn-modelling). The goal is to use the given information to predict whether a bank customer will churn or not.

# Read in the data
data = pd.read_csv('bank_churn.csv')

# Inspect first 5 records
data.head()

# First do some preprocessing. Remove the following columns: RowNumber, CustomerID, Surname. Convert categorical data into dummy variables (with dropping). Split data into 80%-20% train/test sets.
y = data['Exited']

data['HasCrCard'] = data['HasCrCard'].astype(str) # Turn has card to a string so that the function we designed will split correctly
data['IsActiveMember'] = data['IsActiveMember'].astype(str) # Same with is active member feature

# Remove features that will not be used to predict
del data['Exited']
del data['RowNumber']
del data['CustomerId']
del data['Surname']

# Run the function defined earlier on the bank chrun data
data_num, data_cat, cat_index, mixed_df = d_type_split_num_str(data)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(mixed_df, y, test_size=0.2, random_state = 862)

# You will now practice each of the ensemble methods we discussed. Your job is to build an ensemble classifier using each method, and provide me your evaluation (accuracy) on the test set. Remember to conduct the appropriate preprocessing (if needed). You may tune your models if you want.

# ### Voting (Soft) Classifier
# Use 5 classifiers, where two of them needs to be same but different hyperparameters. You can choose your own classifiers. Build a soft voting classifier.
from sklearn.linear_model import LogisticRegression, Lasso, Ridge
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import VotingClassifier # This function is to perform voting classifier
from sklearn.neighbors import KNeighborsClassifier

# Define the individual models
lrRidge = LogisticRegression(solver = 'newton-cg',random_state = 862)
lrLasso = LogisticRegression(penalty = 'l1', solver = 'liblinear', random_state = 862)
GB = GradientBoostingClassifier(random_state = 862)
DT = DecisionTreeClassifier(random_state = 862)
KNN = KNeighborsClassifier()

# Fit the voting classifiers
vcSoft = VotingClassifier(estimators = 
                                    [('lrRidge', lrRidge), 
                                    ('lrLasso', lrLasso), # Technically the same classifier but with lasso (L1) penalty instead of ridge
                                    ('GB', GB), 
                                    ('DT', DT),
                                    ('KNN',KNN)],
                                    voting = 'soft', n_jobs = 2)

# Set hyperparameters for the weak learners
param = {'lrRidge__C':np.geomspace(.00001,10000,3), 
        'lrLasso__C':np.geomspace(.00001,10000,3),
        'GB__learning_rate':np.geomspace(.01,.8,3),
         'DT__max_depth':np.linspace(5,30,3).astype(int),
         'KNN__n_neighbors': np.linspace(5,30,3).astype(int)}

gridSoft = GridSearchCV(estimator = vcSoft, param_grid = param, cv=2, n_jobs=-2, scoring='accuracy')
gridSoft.fit(X_train, y_train)
print(gridSoft.best_score_) # best training score .85975
print(gridSoft.best_params_) # {'DT__max_depth': 5, 'GB__learning_rate': 0.0894427190999916, 'KNN__n_neighbors': 5, 'lrLasso__C': 1e-05, 'lrRidge__C': 0.31622776601683794}
print(gridSoft.score(X_test,y_test)) #.8525

# ### Voting (Hard) Classifier
# Now do the same, but with a hard voting classifier. Compare the result with the soft classifier.

# Dictionary of the classifiers with their best parameters as found by the gridsearchCV above
c_dict = {'lrRidge': LogisticRegression(C = 10000, solver = 'newton-cg',random_state = 862),
                   'lrLasso' : LogisticRegression(C = 1e-05, penalty = 'l1', solver = 'liblinear', random_state = 862),
                        'GB' : GradientBoostingClassifier(learning_rate = .8, random_state = 862),
                        'DT' : DecisionTreeClassifier(max_depth = 5, random_state = 862),
                       'KNN' : KNeighborsClassifier(n_neighbors=5)}
# Initiate the hard Voting Classifier 
vcHard = VotingClassifier(estimators = [
                                    ('lrRidge', c_dict['lrRidge']), 
                                    ('lrLasso', c_dict['lrLasso']), 
                                    ('GB', c_dict['GB']), 
                                    ('DT', c_dict['DT']),
                                    ('KNN',c_dict['KNN'])
                                    ]
                      , voting = 'hard', n_jobs = 2)

vcHard.fit(X_train, y_train)
print('Voting Classifier Training Score: ',vcHard.score(X_train,y_train)) # Training Score of .8695 for hard voting with the best parameters found in the soft voting gridsearch 
print('Voting Classifier Test Score: ',vcHard.score(X_test,y_test))   # Test score of     .846

# As a comparison, let's also fit the individual models and see if there's really an improvement with the voting classifier.

for c in c_dict:
  c_dict[c].fit(X_train, y_train)
  print(c, ' Test Score: ', c_dict[c].score(X_test,y_test))

# lrRidge  Test Score:  0.799
# lrLasso  Test Score:  0.7875
# GB       Test Score:  0.8335
# DT       Test Score:  0.844
# KNN      Test Score:  0.837

# **Your Observation:** 

# Soft voting did a slightly better job at predicting on the testing set, slightly less than 1% better. 
# I think that makes sense though as taking the average score for each model is a bit more dynamic in that it is giving each score a weight VS
# hard voting which is not a weighted score so to speak. Its more of a pass fail type of method.
# I also think its quite interesting that the indvidual scores are all < the voting score. That is not exactly intuitive at all but extremly cool.
# I wonder if that is the case for all voting model combos

# ### Bagged Logistic Regression
# Now fit a bagged model, using logistic regression as base. You can choose what logistic regression to use.

from sklearn.ensemble import BaggingClassifier

# Instantiate the model
bag_reg = BaggingClassifier(LogisticRegression(max_iter = 1000, random_state = 862), n_estimators = 100)

param = {'base_estimator__C' : np.geomspace(.00001,10000,30)}

bag_reg_cv = GridSearchCV(estimator = bag_reg, param_grid = param, cv=2, n_jobs=-2, scoring='accuracy')
bag_reg_cv.fit(X_train,y_train)
print('Bagged Training Score: ',bag_reg_cv.best_score_ )# .7995
print('Bagging with Logistic Regression Test Score : ',bag_reg_cv.score(X_test,y_test))
print(bag_reg_cv.best_params_) # {'base_estimator__C': 3.856620421163472}

# **Your Observation:**  

# Upsetting that the bagging classifier with Logistic Regression did not preform better however,
# the results are right in line the non bagged Logistic Regression score above

# ### XGBoost
# Now do the same, but with XGBoost

from xgboost import XGBClassifier

# Instantiate the model
pipe = Pipeline([('clf',XGBClassifier(n_estimators = 100, n_jobs = -2, random_state = 862))])

params = {'clf__learning_rate':np.geomspace(.00001,.1,5),
          'clf__reg_lambda' :np.geomspace(.0001,10000,5),
          'clf__max_depth':np.geomspace(3,30,5).astype(int)}

# Fit the model

xgbclass = GridSearchCV(pipe, params, cv=5, n_jobs=-1)
xgbclass.fit(X_train, y_train)
print('Training Score: ',xgbclass.best_score_) # .865625
print('Best Params: ',xgbclass.best_params_) # {'clf__learning_rate': 0.1, 'clf__max_depth': 3, 'clf__reg_lambda': 0.01}
print('Test Score: ', xgbclass.score(X_test,y_test)) # .8575

# **Your Observation:** 

# Very happy to see that this is our best test score yet! I think that the concept of XGBoost is extremly powerful in that it is using residuals for the ensemble method
# I would also like to tune these and the other hyperparameters in the other models more now that I have a ball park that they are doing well.
# Meaning my parameter search is quite large in scale and now that I know which end of the scale the best results are within I could search closer to those values

# ### Light GBM
# Repeat with Light GBM


from lightgbm import LGBMClassifier

# Instantiate the model

pipe = Pipeline([('clf',LGBMClassifier(n_estimators=100,random_state = 862))])

params = {'clf__learning_rate':np.geomspace(.001,.1,3),
          'clf__reg_lambda' :np.geomspace(.001,100,3),
          'clf__max_depth':np.geomspace(3,30,5).astype(int),
          'clf__num_leaves':np.geomspace(3,30,5).astype(int)}

# Fit the model

lgbmc = GridSearchCV(pipe, params, cv=5, n_jobs=-1)
lgbmc.fit(X_train, y_train)
print('Training Score: ',lgbmc.best_score_) # .866
print('Best Params: ',lgbmc.best_params_) # {'clf__learning_rate': 0.1, 'clf__max_depth': 16, 'clf__num_leaves': 16, 'clf__reg_lambda': 0.001}
print('Test Score: ', lgbmc.score(X_test,y_test)) #.856


# **Your Observation:** 

# Light GBM is also preforming very well!
# It is another variation of XGboost so that makes sense that it is very close in score. I would like to continue to play with the hyperparameters as I am sure
# I could squezze out a little bit better scores with a deeper search closer to where the best params are being shown in this search space above

# ### Stacking
# Lastly, do this with Stacking. You may use the same models you used from the voting classifiers. Choose your own blender function.
from sklearn.svm import LinearSVC

# Split the data evenly for weak learners training and blender training
X_train_weak, X_blend, y_train_weak, y_blend = train_test_split(mixed_df, y, test_size=0.5, random_state = 862)
# Split the blender training for hyperparameter tuning with cross validation
X_blend_train, X_blend_test, y_blend_train, y_blend_test = train_test_split(X_blend, y_blend, test_size=0.2, random_state = 862)

# Create empty dataframes to store predictions
predictions_train = pd.DataFrame()
predictions_test = pd.DataFrame()

# Iterate through the same tuned models used with the voting classifiers above
for c in c_dict:
  c_dict[c].fit(X_train_weak, y_train_weak) # Train weak learners
  predictions_train[c] = c_dict[c].predict(X_blend_train) # Predict on the blender training set
  predictions_test[c] = c_dict[c].predict(X_blend_test) # Predict on the blender testing set

# Scale both prediction sets for use in models below
StandardScaler().fit_transform(predictions_train)
StandardScaler().fit_transform(predictions_test)

# Drop/reset the indecies to make pd.concat work properly
X_blend_train.reset_index(drop = True, inplace = True)
X_blend_test.reset_index(drop = True, inplace = True)
predictions_train.reset_index(drop = True, inplace = True)
predictions_test.reset_index(drop = True, inplace = True)

# Concatenate the original data with the predictions for blender function to predict on
X_blend_train_concat = pd.concat([X_blend_train,predictions_train],axis = 1,ignore_index=True)
X_blend_test_concat = pd.concat([X_blend_test,predictions_test],axis = 1,ignore_index=True)

# Create the classifier pipeline with hyperparameter search
pipe = Pipeline([('clf',LinearSVC(max_iter = 10000,random_state = 862))])

params = {'clf__C':np.geomspace(.0001,100000,30)}

# Initiate the model
blend = GridSearchCV(param_grid=  params, estimator= pipe, scoring = 'accuracy')
blend.fit(X_blend_train_concat,y_blend_train) # Fit the CV hyperparameter search models
print(blend.best_params_) # {'clf__C': 0.007278953843983154}
print(blend.best_score_) # .857
print('Stacking Test Score: ',blend.score(X_blend_test_concat,y_blend_test)) # .865

# Observations:

# Again the score is very close to the XGBoost and LightGBM. However I do not love the fac that I am getting a convergence warning.
# To remedy I made sure that I scaled the prediction results with StandardScaler() AND increased the max iter to 10,000.
# I think this could be a symptom that LinearSVC just isnt a great choice for a blender. I will try a few others.

# Logistc Regression as the blender

pipe = Pipeline([('clf',LogisticRegression(max_iter = 10000,random_state = 862))])

params = {'clf__C':np.geomspace(.0001,1,30)}

blend = GridSearchCV(param_grid=  params, estimator= pipe, scoring = 'accuracy')
blend.fit(X_blend_train_concat,y_blend_train)
print(blend.best_params_) # {'clf__C': 0.5298316906283708}
print(blend.best_score_) #.8564
print('Stacking Test Score: ',blend.score(X_blend_test_concat,y_blend_test)) #.866

# Marginally better than LinearSVC

# XGBoost as the blender

pipe = Pipeline([('clf',XGBClassifier(n_estimators = 100, n_jobs = -2, random_state = 862))])

params = {'clf__learning_rate':np.geomspace(.01,.1,3),
          'clf__reg_lambda' :np.geomspace(.0001,1,3),
          'clf__max_depth':np.geomspace(3,10,3).astype(int)}

blend = GridSearchCV(param_grid=  params, estimator= pipe, scoring = 'accuracy')
blend.fit(X_blend_train_concat,y_blend_train)
print(blend.best_params_)
print(blend.best_score_)
print('Stacking Test Score, XGBoost as blender: ',blend.score(X_blend_test_concat,y_blend_test)) #.869

# Marginally better than Logistic Regression

pipe = Pipeline([('clf',LGBMClassifier(n_estimators=100,random_state = 862))])

params = {'clf__learning_rate':np.geomspace(.001,.1,3),
          'clf__reg_lambda' :np.geomspace(.001,100,3),
          'clf__max_depth':np.geomspace(3,30,5).astype(int),
          'clf__num_leaves':np.geomspace(3,30,5).astype(int)}

blend = GridSearchCV(param_grid=  params, estimator= pipe, scoring = 'accuracy')
blend.fit(X_blend_train_concat,y_blend_train)
print(blend.best_params_)
print(blend.best_score_)
print('Stacking Test Score, LGBM as blender: ',blend.score(X_blend_test_concat,y_blend_test)) #.869

# Final thoughts

# I think that stacking is a very intuitively intelligent way to build a model. Using weak learners with a final blendder is genious.
# Lastly, while the boosting techniques did not increase testing score crazily, we can see that an extra 2% has been added by using light GBM.
# I am extremly glad I have taken the time to work through this problem and see first hand how well these methods work. Great excersize!